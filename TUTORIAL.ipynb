{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3b36d6-ea7c-4862-9f0e-107a179569d4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# [Auditory modeling in PyTorch](https://github.com/msaddler/auditory_model_tutorial)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/msaddler/auditory_model_tutorial/blob/main/TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Copyright (c) 2025 msaddler. MIT license.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0be74c-33cc-4c1f-95ab-0c760207acd3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import standard Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c0332-b984-4ba4-830d-a577924d9ab1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8244754-7c80-4da1-a58c-2a55ce9c1c8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If running this notebook in Google Colab, run the cell below to clone the [**`auditory_model_tutorial`**](https://github.com/msaddler/auditory_model_tutorial) repository into the current runtime and set it to be the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cq3Tq10TRzkw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "cq3Tq10TRzkw",
    "outputId": "9cb5c6fc-2268-4c63-9dd1-3a8fb8256c33",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/msaddler/auditory_model_tutorial.git\n",
    "%cd auditory_model_tutorial\n",
    "!ls -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95130f01-5118-4ff1-85d9-47f42a0f238d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import [**utils.py**](https://github.com/msaddler/auditory_model_tutorial/blob/main/utils.py) and [**modules.py**](https://github.com/msaddler/auditory_model_tutorial/blob/main/modules.py) as Python modules. These scripts contain useful functions and classes that can be called from this notebook once imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37466ab6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "37466ab6",
    "outputId": "4462c68d-26e4-4904-b08f-3416bbf61433",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491edd5-ec55-4247-b1e4-de37015bb956",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Check if a GPU (optional) is available in the current runtime and define the `device` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9186489-7678-4531-b771-cbc5315d2735",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da3747-cc88-4443-a3a1-e3983a4d945b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This repository includes a small dataset of example sound files (speech waveforms stored as wav files), which can be loaded and played in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8acd5-525a-4ecd-9bca-7d342dda9ea7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_filename = glob.glob(\"data/*.wav\")\n",
    "print(f\"Found {len(list_filename)} wav files in the data directory\")\n",
    "\n",
    "filename = list_filename[0]\n",
    "x, sr = sf.read(filename)  # Use `soundfile` package to load a wav file\n",
    "print(f\"Loaded `{filename}`: {x.shape=}, {x.dtype=}, {sr=} Hz\")\n",
    "\n",
    "ipd.display(ipd.Audio(rate=sr, data=x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78947725-14d9-4b65-b797-417f6fcb228a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The [**`utils`**](https://github.com/msaddler/auditory_model_tutorial/blob/main/utils.py) module contains helper functions for manipulating and visualizing audio signals. Use the `utils.make_spectrogram_plot` function to plot the sound waveform and spectrogram of the file loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a68559",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "editable": true,
    "id": "74a68559",
    "outputId": "c059ba92-2a65-4213-d753-1860f423d1c9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = utils.make_spectrogram_plot(\n",
    "    x,\n",
    "    sr,\n",
    "    figsize=(8, 4),\n",
    "    # nfft=512,\n",
    "    # noverlap=256,\n",
    "    str_title=\"Spectrogram of example sound waveform\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fe435-d7de-4dc4-8ff8-6bd5cfaa3ecb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The [**`utils`**](https://github.com/msaddler/auditory_model_tutorial/blob/main/utils.py) module also contains helper functions for generating simple audio signals. Here, we generate a harmonic complex tone, compute its power spectrum, and plot it on a log-scale frequency axis. The [**`matplotlib.pyplot`**](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html) Python package (abbreviated `plt`) offers a MATLAB-like API for plotting. I have provided the `utils.format_axes` helper function to set many axes-related parameters in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bb830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "editable": true,
    "id": "856bb830",
    "outputId": "9ccd11d2-0e09-4cd5-d356-76ab07f86fdb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sr = 20e3\n",
    "x = utils.harmonic_complex_tone(\n",
    "    sr=sr,\n",
    "    dur=0.5,\n",
    "    f0=220,\n",
    "    phase=\"sine\",\n",
    "    harmonics=np.arange(1, 11),\n",
    "    amplitudes=1,\n",
    ")\n",
    "x = utils.set_dbspl(x, 60.0)  # Scale tone to have a sound level of 60 dB re 20e-6 Pa\n",
    "\n",
    "ipd.display(ipd.Audio(rate=sr, data=x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fxx, pxx = utils.periodogram(x, sr=sr, scaling=\"spectrum\")\n",
    "ax.plot(fxx, pxx, color=\"k\", lw=1)\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    str_title=\"Harmonic complex tone\",\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    xscale=\"log\",\n",
    "    yscale=\"linear\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-60, None],\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# # Alternatively, use `utils.make_periodogram_plot` to plot the power spectrum\n",
    "# fig, ax = utils.make_periodogram_plot(x, sr, str_title=\"Harmonic complex tone\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de4744-cdfa-45be-8d62-cf540b0a4a32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So far, we have been considering sound waveforms as NumPy arrays, which are the fundamental datatype for numerical processing in Python. The fundamental datatype for PyTorch is the tensor. Tensors are similar to NumPy arrays, but include support for GPU-acceleration and automatic differentiation. Converting between NumPy arrays and Torch tensors is easy, but requires some attention to device placement and datatypes. PyTorch's [documentation](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) offers a more comprehensive overview of tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46659a88-e074-4b04-abdd-fc57cbafc37e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "impulse_numpy = np.zeros(int(1.0 * sr))  # Initialize 1 second of zeros\n",
    "impulse_numpy[0] = 1  # Set the first value to be 1 to make an impulse\n",
    "\n",
    "# When defining a tensor, it is good practice to specify the device and the datatype\n",
    "impulse_torch = torch.as_tensor(impulse_numpy, device=device, dtype=torch.float32)\n",
    "print(impulse_torch)\n",
    "\n",
    "impulse_torch.detach().cpu().numpy()  # Convert a tensor to a NumPy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4313e4f-63a8-417c-aa16-bfe9452f04d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Components of a PyTorch cochlear model\n",
    "\n",
    "Tensors can be be processed with PyTorch functions and modules. In the [**`modules.py`**](https://github.com/msaddler/auditory_model_tutorial/blob/main/modules.py) script, I have implemented a `GammatoneFilterbank` class, which we will use as the first stage of our simple auditory nerve model.\n",
    "\n",
    "Because PyTorch is designed for parallel numerical processing, this filterbank (like most models in PyTorch) expects *batched* rather than *single* inputs:\n",
    "```\n",
    "x.shape = [time]  <-- single input sound waveform\n",
    "x.shape = [batch, time]  <-- batch of input sound waveforms\n",
    "```\n",
    "\n",
    "To run a single sound waveform through a model, we simply need to add a batch axis:\n",
    "```\n",
    "x = torch.zeros(n)  <-- has shape [n]\n",
    "x = x[None, ...]  <-- has shape [1, n]\n",
    "x = x[0, ...]  <-- back to shape [n]\n",
    "```\n",
    "\n",
    "As an alternative to this odd `[None, ...]` notation, you can also use `unsqueeze`/`squeeze`:\n",
    "```\n",
    "x = torch.zeros(n) <-- has shape [n]\n",
    "x = x.unsqueeze(dim=0) <-- has shape [1, n]\n",
    "x = x.squeeze(dim=0)  <-- back to shape [n]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ab261",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "editable": true,
    "id": "053ab261",
    "outputId": "fc4ffaa8-4cee-456b-e180-2f88e1290943",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our filterbank will have 50 center frequencies, spaced uniformly on an ERB scale\n",
    "cfs = utils.erbspace(8e1, 8e3, 50)\n",
    "# Construct the filterbank object with specified parameters\n",
    "filterbank = modules.GammatoneFilterbank(\n",
    "    sr=sr,\n",
    "    fir_dur=0.05,  # Filterbank uses an FIR approximation (50 ms impulse responses is sufficient)\n",
    "    cfs=cfs,\n",
    "    dtype=torch.float32,\n",
    "    bw_mult=1,\n",
    ")\n",
    "# Assign the filterbank object to a device\n",
    "filterbank = filterbank.to(device)\n",
    "\n",
    "# Apply the filterbank to our impulse tensor (note we added a batch axis)\n",
    "impulse_response_torch = filterbank(impulse_torch[None, ...])\n",
    "print(impulse_response_torch.shape)\n",
    "# And remove the added batch axis to get an output with shape [freq, time]\n",
    "impulse_response_torch = impulse_response_torch[0, ...]\n",
    "print(impulse_response_torch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ff6c1-4ab1-4ab1-bc88-d16edd6e1815",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Plot the power spectra of the filterbank's impulse responses. The `utils.make_periodogram_plot` function will plot the power spectrum of each frequency channel if the input has shape `[frequency_channels, time]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea41a41-4ae4-4e6d-b99d-b371d58da175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, convert the impulse response tensor to a NumPy array by detaching\n",
    "# it from the torch graph, moving it to the CPU, and calling .numpy()\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "print(impulse_response.shape)\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    impulse_response,\n",
    "    sr,\n",
    "    str_title=\"Gammatone filterbank impulse responses\",\n",
    "    ylimits=[-30, None],\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392da773-a432-4488-a2b8-acf75f745bef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This linear filterbank approximates the **cochlea's frequency selectivity**. The input represents the instantaneous pressure of a sound waveform and the output represents the instantaneous displacement of the basilar membrane at different points along its length (i.e, the \"cochlear frequency axis\"). We can pass arbitrary sounds through this filterbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf4de5-0096-4659-8988-d9994002cde6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate 1 second of white noise and plot the power spectra of the filtered subbands\n",
    "x = torch.randn(1, int(sr), device=device, dtype=torch.float32)\n",
    "y = filterbank(x)[0].detach().cpu().numpy()\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    y,\n",
    "    sr,\n",
    "    str_title=\"White noise filtered by the gammatone filterbank\",\n",
    "    ylimits=[-30, None],\n",
    "    xlimits=[0, sr / 2],\n",
    "    xscale=\"linear\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abee7a4-f4e0-4391-98ed-d7d33ba14ebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The next stage of our peripheral auditory model is a very crude model of **inner hair cell (IHC) transduction**. Here, we simply use half-wave rectification to abstractly convert *instantaneous basilar membrane displacement* (the filterbank subbands) to *instantaneous IHC membrane potential* (the rectified subbands).\n",
    "\n",
    "In NumPy / MATLAB, half-wave rectification is easily implemented as `x[x < 0] = 0`. In PyTorch, to avoid disrupting gradient tracking, we will use the `torch.nn.ReLU` class, which implements the \"Rectified Linear Unit (ReLU)\" activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1f8c0-e0d4-4c79-b097-6b594f9e4d82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "editable": true,
    "id": "b1a1f8c0-e0d4-4c79-b097-6b594f9e4d82",
    "outputId": "b356276a-e0f5-473b-dafd-99722b06a39c",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "half_wave_rectification = torch.nn.ReLU()\n",
    "x = np.arange(-9, 10)\n",
    "x_rectified = half_wave_rectification(torch.as_tensor(x)).detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, x_rectified)\n",
    "ax.axvline(0, color=\"k\", lw=0.2)\n",
    "ax.axhline(0, color=\"k\", lw=0.2)\n",
    "ax.axis(\"square\")\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    xlimits=[-10, 10],\n",
    "    ylimits=[-10, 10],\n",
    "    str_xlabel=\"Basilar membrane displacement (a.u.)\",\n",
    "    str_ylabel=\"Inner hair cell membrane potential (a.u.)\",\n",
    "    str_title=\"IHC transduction crudely modeled\\nas half-wave rectification (ReLU)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d275da3-4238-48da-b42a-977283ff1b34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To more accurately model the IHC membrane potential, we also need to apply a low-pass filter.\n",
    "\n",
    "The **IHC membrane potential** is sluggish relative to the mechanical vibration of the basilar membrane. At low frequencies, the IHC potential tracks the individual oscillations of the basilar membrane, but at higher frequencies the IHC potential cannot keep up. The IHC membrane acts as a lowpass filter that restricts the upper frequency limit of phase locking in the auditory nerve.\n",
    "\n",
    "The `modules.IHCLowpassFilter` class implements a low-pass filter that can operate on inputs with shape `[batch, time]` or `[batch, freq, time]`. In the latter case, the same filter is applied along the time axis to each frequency channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a13ca5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "editable": true,
    "id": "07a13ca5",
    "outputId": "3a1f179f-a10a-47bc-ef7b-9cc93b070080",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ihc_lowpass_filter = modules.IHCLowpassFilter(\n",
    "    sr_input=sr,\n",
    "    sr_output=sr,\n",
    "    fir_dur=0.05,\n",
    "    cutoff=3e3,\n",
    "    order=7,\n",
    ").to(device)\n",
    "\n",
    "impulse_response_torch = ihc_lowpass_filter(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "print(impulse_response.shape)\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    impulse_response,\n",
    "    sr,\n",
    "    str_title=\"Low-pass filter to limit auditory nerve phase locking\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68285100-2b6e-46b3-80bc-6498b5dcf385",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The last stage of the simple peripheral model will convert the half-wave rectified and low-pass filtered subbands to **instantaneous auditory nerve firing rates**. This is accomplished with a sigmoid function whose threshold and dynamic range can be set in dB SPL. Applying a sigmoid function at the output lets us crudely model loss of audibility at low sound levels and saturation at high sound levels.\n",
    "\n",
    "The parameters of this rate-level function are somewhat arbitrary and  ignore much of the complexity of real auditory nerve fibers. Here, we make some simplifying (and incorrect) assumptions about a population of healthy auditory nerve fibers:\n",
    "- Threshold = 0 dB SPL\n",
    "- Dynamic range = 80 dB\n",
    "- Spontaneous activity = 0 spikes/s\n",
    "- Maximum activity = 250 spikes/s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309fb89-0217-457e-aa81-9e19abc35989",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "editable": true,
    "id": "5309fb89-0217-457e-aa81-9e19abc35989",
    "outputId": "4b44649b-5ac2-4a1a-c69f-238d9cfdee49",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rate_level_function = modules.SigmoidRateLevelFunction(\n",
    "    rate_spont=0.0,\n",
    "    rate_max=250.0,\n",
    "    threshold=0.0,\n",
    "    dynamic_range=80.0,\n",
    "    dynamic_range_interval=0.95,\n",
    "    dtype=torch.float32,\n",
    ").to(device)\n",
    "x_db = np.arange(-20, 121)\n",
    "x_pa = 20e-6 * (10 ** (x_db / 20))\n",
    "x_pa_torch = torch.as_tensor(\n",
    "    x_pa,\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "y_torch = rate_level_function(x_pa_torch[None, None, :])[0, 0, :]\n",
    "y = y_torch.detach().cpu().numpy()\n",
    "ax.plot(x_db, y)\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    str_xlabel=\"Sound pressure level (dB SPL)\",\n",
    "    str_ylabel=\"Auditory nerve firing rate (spike/s)\",\n",
    "    str_title=\"Sigmoid rate-level function (healthy ear)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69a3d7-9b5f-410d-99f7-12544f0e4dc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In the healthy ear, **outer hair cells (OHCs) act as a cochlear amplifier**, boosting responses to quiet sounds. Sensorineural hearing loss often involves the death or dysfunction of OHCs, resulting in reduced sensitivity to quiet sounds (elevated thresholds) and a smaller dynamic range. We can simulate these effects by adjusting the rate-level function parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed535c1f-f684-45c9-9053-75fdebdc575e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "healthy_threshold = 0.0\n",
    "healthy_dynamic_range = 80.0\n",
    "for dbhl in np.arange(0, 75, 10):\n",
    "    rate_level_function = modules.SigmoidRateLevelFunction(\n",
    "        rate_spont=0.0,\n",
    "        rate_max=250.0,\n",
    "        threshold=healthy_threshold + dbhl,  # Simply increase the threshold\n",
    "        dynamic_range=healthy_dynamic_range - dbhl,  # And decrease the dynamic range\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    x_db = np.arange(-20, 121)\n",
    "    x_pa = 20e-6 * (10 ** (x_db / 20))\n",
    "    x_pa_torch = torch.as_tensor(\n",
    "        x_pa,\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    y_torch = rate_level_function(x_pa_torch[None, None, :])[0, 0, :]\n",
    "    y = y_torch.detach().cpu().numpy()\n",
    "    ax.plot(x_db, y, label=f\"{dbhl} dB HL\")\n",
    "ax.legend(title=\"Hearing loss\")\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    str_xlabel=\"Sound pressure level (dB SPL)\",\n",
    "    str_ylabel=\"Auditory nerve firing rate (spike/s)\",\n",
    "    str_title=\"Sigmoid rate-level function (loss of cochear amplification)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0f5d2-dd4f-4546-b744-22293dfc3086",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Assembling the full cochlear model\n",
    "\n",
    "Now that we have examined each of the cochlear model stages individually, it is time to assemble the full cochlear model into a single PyTorch module.\n",
    "\n",
    "A skeleton for the `CochlearModel` class is provided below. If you are familiar with Python classes, you will notice the skeleton is subclassing the [**`torch.nn.Module`**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class. This class is the base class for all models in PyTorch.\n",
    "\n",
    "Because our model is a subclass of `torch.nn.Module`, we only need to implement two methods (`__init__` and `forward`) to obtain a fully-functioning PyTorch model. The `__init__` method sets up the model and is where you should define each of the model components. The `forward` method defines the computations that should be performed each time the model is called. Your `forward` pass should successively apply the model components defined in the `__init__` method.\n",
    "\n",
    "The convenience of PyTorch is that if we want to compute gradients from or through our model, we do not need to implement the `backward` pass ourselves. The derivatives will all be computed / tracked automatically!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1858f-382a-4ece-9dd0-02826b022ae7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CochlearModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sr_input=20000,\n",
    "        sr_output=10000,\n",
    "        fir_dur=0.05,\n",
    "        cfs=utils.erbspace(8e1, 8e3, 100),\n",
    "        bw_mult=1.0,\n",
    "        threshold=0.0,\n",
    "        dynamic_range=80.0,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple cochlear model in PyTorch.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        sr_input (int): sampling rate of the input sound waveform\n",
    "        sr_output (int): sampling rate of the output representation\n",
    "        fir_dur (float): duration of finite impulse responses for filtering (s)\n",
    "        cfs (np.ndarray): characteristic frequencies of the cochlear filters (Hz)\n",
    "        bw_mult (float): scaling factor applied to cochlear filter bandwidths\n",
    "        threshold (float): absolute threshold of auditory nerve fibers (dB SPL)\n",
    "        dynamic_range (float): auditory nerve fiber dynamic range (dB)\n",
    "        dtype (torch.dtype): datatype for internal intensors and inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sr_input = sr_input\n",
    "        self.sr_output = sr_output\n",
    "        self.cfs = cfs\n",
    "\n",
    "        # Initialize the model stages here!\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run the cochlear model on a batch of input sound waveforms.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x (torch.Tensor): input sound waveforms with shape [batch, time]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x (torch.Tensor): output of cochlear model with shape [batch, freq, time]\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the forward pass here!\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59ba90-03d6-4c19-a4bd-110008896b81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test your cochlear model implementation in the cell below:\n",
    "\n",
    "1. Construct a `cochlear_model` object with a call to the `CochlearModel` class\n",
    "2. Generate or load a sound waveform `x`\n",
    "3. Convert `x` to a tensor with shape `[batch=1, time]`\n",
    "4. Apply `cochlear_model` to `x` to produce `x_nervegram` with shape `[batch=1, freq, time]`\n",
    "5. Convert `x_nervegram` to a NumPy array with shape `[freq, time]`\n",
    "6. Visualize `x_nervegram` using `utils.make_nervegram_plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4728d-1b18-4c04-ab9c-ed919495e796",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cochlear_model = CochlearModel(\n",
    "    sr_input=20e3,\n",
    "    sr_output=10e3,\n",
    "    cfs=utils.erbspace(8e1, 8e3, 100),\n",
    ").to(device)\n",
    "\n",
    "x = utils.harmonic_complex_tone(\n",
    "    sr=cochlear_model.sr_input,\n",
    "    dur=0.05,\n",
    "    f0=220,\n",
    "    phase=\"sine\",\n",
    "    harmonics=np.arange(1, 31, 1),\n",
    ")\n",
    "x = utils.set_dbspl(x, 60)\n",
    "# x, _ = sf.read(\"data/000.wav\")\n",
    "\n",
    "x_tensor = torch.as_tensor(x, device=device).float()[None, ...]\n",
    "x_nervegram_tensor = cochlear_model(x_tensor)[0]\n",
    "x_nervegram = x_nervegram_tensor.detach().cpu().numpy()\n",
    "print(x_nervegram.shape)\n",
    "\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram,\n",
    "    sr_waveform=cochlear_model.sr_input,\n",
    "    sr_nervegram=cochlear_model.sr_output,\n",
    "    cfs=cochlear_model.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd0bd0-0c49-479b-a062-4202a90ae98a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Get a sense for how fast the cochlear model can run on the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003b9da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "9003b9da",
    "outputId": "68295ff1-ce01-422a-e873-fc7f839640b8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm.tqdm(range(200))  # tqdm.tqdm wraps an iterable and displays a progress bar\n",
    "for itr in progress_bar:\n",
    "    x_nervegram_tensor = cochlear_model(x_tensor)\n",
    "    progress_bar.set_postfix({\"current_iteration\": itr})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a3b50-9b05-4da9-97a5-134a2e6919f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hearing loss simulation\n",
    "\n",
    "Simulate the peripheral consequences of hearing loss by adjusting the cochlear model parameters. Visualize the effect on the resulting auditory nerve representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e0394-e80e-43cc-9473-f56de52d2cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "editable": true,
    "id": "db7e0394-e80e-43cc-9473-f56de52d2cc8",
    "outputId": "d070d591-f462-4de8-aa7c-76165add1b34",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfs = utils.erbspace(8e1, 8e3, 100)\n",
    "\n",
    "# Construct a healthy cochlear model\n",
    "cochlear_model_healthy = CochlearModel(\n",
    "    sr_input=sr,\n",
    "    cfs=cfs,\n",
    "    threshold=0.0,\n",
    "    dynamic_range=80.0,\n",
    ").to(device)\n",
    "\n",
    "# Construct an impaired cochlear model\n",
    "cochlear_model_impaired = CochlearModel(\n",
    "    sr_input=sr,\n",
    "    cfs=cfs,\n",
    "    threshold=60.0,\n",
    "    dynamic_range=20.0,\n",
    ").to(device)\n",
    "\n",
    "# Define a stimulus to play to both cochlear models\n",
    "x = utils.harmonic_complex_tone(\n",
    "    sr=cochlear_model_healthy.sr_input,\n",
    "    dur=0.05,\n",
    "    f0=220,\n",
    "    phase=\"sine\",\n",
    "    harmonics=np.arange(1, 31, 1),\n",
    ")\n",
    "x = utils.set_dbspl(x, 70)\n",
    "\n",
    "# Run the stimulus through both cochlear models\n",
    "x_tensor = torch.as_tensor(x, device=device, dtype=torch.float32)[None, ...]\n",
    "x_nervegram_healthy = cochlear_model_healthy(x_tensor)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired = cochlear_model_impaired(x_tensor)[0].detach().cpu().numpy()\n",
    "\n",
    "# Visualize both auditory nerve representations\n",
    "print(\"Healthy cochlear model\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_healthy,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_impaired,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a8afe-a8f7-4ee6-8d8d-9ec332b3e63f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hearing aid optimization objective\n",
    "\n",
    "A reasonable objective for a hearing aid would be to process audio in such a way that the auditory nerve representation of an impaired ear is maximally similar to that of a healthy ear. How can we implement this notion quantitatively as a *loss function*?\n",
    "\n",
    "Define a `loss_function` that takes as input an unprocessed sound waveform `x` and a processed sound_waveform `x_aided`. The function should return some [**measure of distance**](https://pytorch.org/docs/stable/nn.functional.html#loss-functions) between the healthy auditory nerve representation of `x` and the impaired auditory nerve representation of `x_aided`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694275c-75af-49ba-a04f-e06b9d733aea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function(x, x_aided):\n",
    "    # Implement the loss function here!\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df121f98-080c-43ba-b80d-81486cd992ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will first consider the **simplest possible hearing aid**: linear amplification applied to the entire sound waveform. Here, there is only a single parameter to optimize (the gain), so we can easily find the optimal value with brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcaaed-fb81-48be-97a7-f603eb823f3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_db_gain = np.arange(0, 101, 5)\n",
    "list_loss = []\n",
    "for db_gain in tqdm.tqdm(list_db_gain):\n",
    "    amp = np.power(10, db_gain / 20)\n",
    "    loss = loss_function(\n",
    "        x=x_tensor,\n",
    "        x_aided=amp * x_tensor,\n",
    "    ).item()\n",
    "    list_loss.append(loss)\n",
    "\n",
    "best_db_gain = list_db_gain[np.argmin(list_loss)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list_db_gain, list_loss, marker=\".\", color=\"r\", ls=\"-\")\n",
    "ax.plot(\n",
    "    best_db_gain,\n",
    "    np.min(list_loss),\n",
    "    marker=\"o\",\n",
    "    ms=12,\n",
    "    ls=\"\",\n",
    "    mfc=\"none\",\n",
    "    color=\"k\",\n",
    "    label=f\"Optimal gain = {best_db_gain} dB\",\n",
    ")\n",
    "ax.legend()\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    str_title=\"Brute force hearing aid optimization\",\n",
    "    str_xlabel=\"Hearing aid parameter (gain in dB)\",\n",
    "    str_ylabel=\"Loss function (MSE between healthy and\\nimpaired auditory nerve representation)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ab4a7-d887-4df5-a3b4-9295a561fbf5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Visualize the resulting auditory nerve representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4071a-a31a-4b4a-bfef-c5646e4b4e27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "editable": true,
    "id": "7de4071a-a31a-4b4a-bfef-c5646e4b4e27",
    "outputId": "8dc6169b-61cf-4407-f9a3-75936a571ede",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_amp = np.power(10, best_db_gain / 20)\n",
    "\n",
    "x_tensor = torch.as_tensor(x, device=device, dtype=torch.float32)[None, ...]\n",
    "x_nervegram_healthy = cochlear_model_healthy(x_tensor)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired = cochlear_model_impaired(best_amp * x_tensor)[0].detach().cpu().numpy()\n",
    "\n",
    "print(\"Healthy cochlear model with unprocessed input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_healthy,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model with aided input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_impaired,\n",
    "    sr_waveform=cochlear_model_impaired.sr_input,\n",
    "    sr_nervegram=cochlear_model_impaired.sr_output,\n",
    "    cfs=cochlear_model_impaired.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4eccb-bcfe-4a4b-80a3-445c0f39536d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Real hearing losses are rarely uniform across all frequencies. How can we compensate for a more plausible frequency-dependent hearing loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13f0e3-3a9f-43f4-9e8d-090c361b9ea0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for severity in [\"ref\", \"mild\", \"moderate\", \"moderate_severe\"]:\n",
    "    freq, dbhl = utils.get_example_audiogram(severity)\n",
    "    ax.plot(freq, dbhl, marker=\"o\", label=severity)\n",
    "ax.legend(title=\"Hearing loss severity\")\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    ylimits=[90, -10],\n",
    "    xticks=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticklabels=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticks_minor=[],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Hearing threshold elevation (dB HL)\",\n",
    "    str_title=\"Example audiograms\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c93e3-fff5-4f78-89d6-e5bcee7fa90d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Construct a new impaired cochlear model that better approximates a realistic hearing-impaired audiogram. Here we set the model's `threshold` and `dynamic_range` as frequency-specific arrays with the same shape as `cfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2fa0b-e9f6-412c-960a-ac1b24558390",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq, dbhl = utils.get_example_audiogram(severity=\"moderate_severe\")\n",
    "threshold, dynamic_range = utils.map_audiogram_to_rate_level_parameters(\n",
    "    freq=freq,\n",
    "    dbhl=dbhl,\n",
    "    cfs=cfs,\n",
    "    healthy_threshold=healthy_threshold,\n",
    "    healthy_dynamic_range=healthy_dynamic_range,\n",
    ")\n",
    "print(f\"{cfs.shape=}, {threshold.shape=}, {dynamic_range.shape=}\")\n",
    "cochlear_model_impaired = CochlearModel(\n",
    "    sr_input=sr,\n",
    "    cfs=cfs,\n",
    "    threshold=threshold,\n",
    "    dynamic_range=dynamic_range,\n",
    ").to(device)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(freq, dbhl, marker=\"o\", color=\"k\", label=\"Example audiogram\")\n",
    "ax.plot(\n",
    "    cochlear_model_impaired.cfs,\n",
    "    cochlear_model_impaired.rate_level_function.threshold,\n",
    "    marker=\".\",\n",
    "    color=\"r\",\n",
    "    label=\"Rate-level function thresholds at each CF\",\n",
    ")\n",
    "ax.legend()\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    ylimits=[90, -10],\n",
    "    xticks=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticklabels=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticks_minor=[],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Hearing threshold (dB HL)\",\n",
    "    str_title=\"Setting model parameters to match an audiogram\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f1c2-fdfe-451e-b845-a64b713d1c9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hearing aid with frequency-dependent amplification\n",
    "\n",
    "One possibility is to build a hearing aid with frequency-specific gains. We can filter the input sound into different frequency channels, scale the channels with separate gains, and then sum the channels back together. The `modules.HalfCosineFilterbank` class is a reasonable filterbank for decomposing and reconstructing audio in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb910c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "editable": true,
    "id": "73cb910c",
    "outputId": "a80bc177-09dc-4045-8288-c340ba567748",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filterbank = modules.HalfCosineFilterbank(\n",
    "    sr=sr,\n",
    "    cf_low=20,\n",
    "    cf_high=8000,\n",
    "    cf_num=7,\n",
    "    scale=\"log\",\n",
    "    include_highpass=True,\n",
    "    include_lowpass=True,\n",
    ").to(device)\n",
    "\n",
    "impulse_response_torch = filterbank(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "print(impulse_response.shape)\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    impulse_response,\n",
    "    sr,\n",
    "    str_title=\"Half-cosine filterbank\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d81967-602c-4947-9eca-d9777d1ccd7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Since hearing aids can have many tunable parameters (e.g., frequency-specific gains), we need a more scalable optimization method than brute force. Implementing a hearing aid as a PyTorch model with learnable parameters unlocks the tools of machhine learning -- we can use *gradient descent* to efficiently find parameter values that minimize our loss function.\n",
    "\n",
    "Complete the `HearingAid` model class below. The `__init__` method has already been implemented. Only the `forward` method needs to be filled in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92f7ae-d8f7-403c-8295-e8cf345cdc9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HearingAid(torch.nn.Module):\n",
    "    def __init__(self, sr=20e3, channels=7):\n",
    "        \"\"\"\n",
    "        Initialize the hearing aid model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filterbank = modules.HalfCosineFilterbank(\n",
    "            sr=sr,\n",
    "            cf_low=20,\n",
    "            cf_high=sr / 2,\n",
    "            cf_num=channels,\n",
    "            scale=\"log\",\n",
    "            include_highpass=True,\n",
    "            include_lowpass=True,\n",
    "        )\n",
    "        self.gains_db = torch.nn.parameter.Parameter(\n",
    "            data=torch.zeros(self.filterbank.cf_num),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the computations to an input sound `x`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the forward pass here!\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2224ba-712c-4019-9c0f-125f5144e3f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test your `HearingAid` implementation by visualizing the impulse reponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7faf8c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "editable": true,
    "id": "d7faf8c4",
    "outputId": "4fccba92-4837-4562-b43a-106edff0f7bd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hearing_aid = HearingAid(sr=sr, channels=7).to(device)\n",
    "# hearing_aid.gains_db = torch.nn.Parameter(\n",
    "#     5 * torch.randn(*hearing_aid.gains_db.shape, device=device, dtype=torch.float32),\n",
    "# )\n",
    "print(hearing_aid)\n",
    "for n, p in hearing_aid.named_parameters():\n",
    "    print(n, p if p.ndim == 1 else p.shape)\n",
    "\n",
    "impulse_response_torch = hearing_aid(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fxx, pxx = utils.periodogram(impulse_numpy, sr)\n",
    "fyy, pyy = utils.periodogram(impulse_response, sr)\n",
    "ax.plot(fxx, pxx, color=\"k\", label=\"Hearing aid input\")\n",
    "ax.plot(fyy, pyy, color=\"r\", label=\"Hearing aid output\")\n",
    "ax.legend()\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-10, None],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    str_title=\"Hearing aid transfer function before optimization\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31453590-3594-4f7f-8326-ccfdeeda695e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Gradient-based hearing aid optimization \n",
    "\n",
    "We will optimize the parameters of the hearing aid using gradient descent to minimize our loss function.\n",
    "\n",
    "**This cell will run extremely slowly without GPU acceleration**. If a GPU is not available, use a smaller dataset by truncating the number of examples and/or the length of the stimuli.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4b082",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "22e4b082",
    "outputId": "c6971a9c-9575-4bf3-8f38-b56ebef48ae3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset (for time, we use only a small batch of speech signals)\n",
    "x = np.stack(\n",
    "    [sf.read(\"data/{:03d}.wav\".format(_))[0] for _ in range(1)],\n",
    "    axis=0,\n",
    ")\n",
    "x = torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "print(f\"Dataset shape: {x.shape}\")\n",
    "\n",
    "# Define a PyTorch optimizer object and tell it which parameters to update\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=list(hearing_aid.parameters()),\n",
    "    lr=1e-1,  # The learning rate is a hyperparameter (determines gradient descent step size)\n",
    ")\n",
    "\n",
    "# In the optimization loop, we iteratively compute the loss,\n",
    "# calculate the gradients with `loss.backward()`, and then\n",
    "# call `optimizer.step()` to update the parameters.\n",
    "progress_bar = tqdm.tqdm(range(500))\n",
    "for step in progress_bar:\n",
    "    optimizer.zero_grad()\n",
    "    x_aided = hearing_aid(x)\n",
    "    loss = loss_function(x=x, x_aided=x_aided)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    progress_bar.set_postfix({\"loss\": \"{:0.4f}\".format(loss.item())})\n",
    "\n",
    "print(\"hearing_aid parameters after optimization:\")\n",
    "for n, p in hearing_aid.named_parameters():\n",
    "    print(n, p if p.ndim == 1 else p.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e5841-7f12-4810-b388-96d71e54d415",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Visualize the optimized hearing aid's signal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7ecdf-dfa4-4884-b630-9d17fdea195f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "editable": true,
    "id": "4de7ecdf-dfa4-4884-b630-9d17fdea195f",
    "outputId": "324fd35d-cb54-438a-e6ba-57ca7f267543",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "impulse_response_torch = hearing_aid(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fxx, pxx = utils.periodogram(impulse_numpy, sr)\n",
    "fyy, pyy = utils.periodogram(impulse_response, sr)\n",
    "ax.plot(fxx, pxx, color=\"k\", label=\"Hearing aid input\")\n",
    "ax.plot(fyy, pyy, color=\"r\", label=\"Hearing aid output\")\n",
    "ax.legend()\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-10, None],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    str_title=\"Hearing aid transfer function after optimization\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "example_audio = torch.as_tensor(\n",
    "    sf.read(\"data/000.wav\".format(_))[0][None, :],\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "example_audio_aided = hearing_aid(example_audio)\n",
    "example_audio = example_audio[0].detach().cpu().numpy()\n",
    "example_audio_aided = example_audio_aided[0].detach().cpu().numpy()\n",
    "\n",
    "fig, ax_arr = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "t = np.arange(0, len(example_audio)) / sr\n",
    "ax_arr[0].plot(t, example_audio, color=\"k\", alpha=0.5)\n",
    "ax_arr[0].plot(t, example_audio_aided, color=\"r\", alpha=0.5)\n",
    "ax_arr[0] = utils.format_axes(\n",
    "    ax_arr[0],\n",
    "    str_xlabel=\"Time (s)\",\n",
    "    str_ylabel=\"Pressure (Pa)\",\n",
    "    str_title=\"Time domain\",\n",
    ")\n",
    "fxx, pxx = utils.periodogram(example_audio, sr)\n",
    "fyy, pyy = utils.periodogram(example_audio_aided, sr)\n",
    "ax_arr[1].plot(fxx, pxx, color=\"k\", alpha=0.5)\n",
    "ax_arr[1].plot(fyy, pyy, color=\"r\", alpha=0.5)\n",
    "ax_arr[1] = utils.format_axes(\n",
    "    ax_arr[1],\n",
    "    xscale=\"log\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-10, None],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    str_title=\"Frequency domain\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ipd.display(ipd.Audio(rate=sr, data=example_audio))\n",
    "ipd.display(ipd.Audio(rate=sr, data=example_audio_aided))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75062b77-4110-43a2-a56e-59519a9c9ce4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Visualize the resulting auditory nerve representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3a893",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "editable": true,
    "id": "b4b3a893",
    "outputId": "b69422f6-1ee4-4f22-eb66-c862e44d5b21",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = utils.harmonic_complex_tone(\n",
    "#     sr=sr,\n",
    "#     dur=0.05,\n",
    "#     f0=220,\n",
    "#     phase=\"sine\",\n",
    "#     harmonics=np.arange(1, 31),\n",
    "# )\n",
    "# x = utils.set_dbspl(x, 60.0)\n",
    "x, _ = sf.read(\"data/099.wav\")\n",
    "\n",
    "x = torch.as_tensor(x[None, ...], device=device, dtype=torch.float32)\n",
    "x_aided = hearing_aid(x)\n",
    "x_nervegram_healthy = cochlear_model_healthy(x)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired = cochlear_model_impaired(x)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired_aided = cochlear_model_impaired(x_aided)[0].detach().cpu().numpy()\n",
    "x = x[0].detach().cpu().numpy()\n",
    "x_aided = x_aided[0].detach().cpu().numpy()\n",
    "\n",
    "print(\"Healthy cochlear model with unprocessed input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_healthy,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model with unprocessed input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_impaired,\n",
    "    sr_waveform=cochlear_model_impaired.sr_input,\n",
    "    sr_nervegram=cochlear_model_impaired.sr_output,\n",
    "    cfs=cochlear_model_impaired.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model with aided input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x_aided,\n",
    "    nervegram=x_nervegram_impaired_aided,\n",
    "    sr_waveform=cochlear_model_impaired.sr_input,\n",
    "    sr_nervegram=cochlear_model_impaired.sr_output,\n",
    "    cfs=cochlear_model_impaired.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aef374-7f9f-41a2-9912-da8bae28e870",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Another way to achieve frequency-specific linear gain is with convolution in the time domain. The `ConvolutionalHearingAid` class below convolves its inputs with a learnable FIR filter kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b1474-992c-48f5-91c2-ebb115e5c5ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalHearingAid(torch.nn.Module):\n",
    "    def __init__(self, sr=20e3, fir_dur=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the hearing aid model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = modules.AudioConv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=None,\n",
    "            sr=sr,\n",
    "            fir_dur=fir_dur,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the computations to an input sound `x`.\n",
    "        \"\"\"\n",
    "        y = self.conv(x.unsqueeze(1)).squeeze(1)\n",
    "        return y\n",
    "\n",
    "\n",
    "ConvolutionalHearingAid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fc3c7-61de-4032-935e-72dcc98255cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## **Ideas for additional exploration**\n",
    "\n",
    "On first glance, the hearing aid with frequency-specific linear gains appears to do fairly well. The impaired auditory nerve representation of the hearing-aid-processed signal reasonably resembles the healthy auditory nerve representation of the unprocessed signal.\n",
    "\n",
    "But it is far from perfect -- linear amplification cannot compensate for the reduced dynamic range of the impaired ear. Can you design and optimize a hearing aid that further minimizes the loss function?\n",
    "\n",
    "In addition to elevating thresholds and reducing dynamic ranges, loss of OHCs also causes leads to broader cochlear frequency tuning. This can be simulated by specifying `bw_mult` in `modules.GammatoneFilterbank`. Can you learn a hearing aid that helps compensate for this consequence of hearing loss?\n",
    "\n",
    "The \"hearing aids\" in this notebook are toy examples with simple signal processing and very small numbers of parameters. With a bit more compute power and a larger dataset, the same approach can be used to optimize more complex audio-processing systems, such as deep artificial neural networks. See [**`torchaudio.models`**](https://pytorch.org/audio/main/models.html) for ready-to-use PyTorch neural network models.\n",
    "\n",
    "Here, our optimization objective is to restore healthy auditory nerve representations in a very simple model of the impaired auditory nerve. Is this a good \"loss function\" for a hearing aid? How might we improve upon it?\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
