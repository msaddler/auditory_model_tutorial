{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3b36d6-ea7c-4862-9f0e-107a179569d4",
   "metadata": {},
   "source": [
    "## [Auditory modeling in PyTorch](https://github.com/msaddler/auditory_model_tutorial)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/msaddler/auditory_model_tutorial/blob/main/TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Copyright (c) 2025 msaddler. MIT license.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c0332-b984-4ba4-830d-a577924d9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import standard Python packages.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cq3Tq10TRzkw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cq3Tq10TRzkw",
    "outputId": "9cb5c6fc-2268-4c63-9dd1-3a8fb8256c33"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If running this notebook in Google Colab, run this cell to\n",
    "clone the `auditory_model_tutorial` repository into the \n",
    "current runtime and set it to be the working directory.\n",
    "\"\"\"\n",
    "\n",
    "!git clone https://github.com/msaddler/auditory_model_tutorial.git\n",
    "%cd auditory_model_tutorial\n",
    "!ls -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37466ab6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37466ab6",
    "outputId": "4462c68d-26e4-4904-b08f-3416bbf61433"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import `filters.py`, `modules.py`, and `utils.py` as Python modules.\n",
    "These scripts contain useful functions and classes that can be called\n",
    "from this notebook once imported.\n",
    "\"\"\"\n",
    "\n",
    "import utils\n",
    "import filters\n",
    "import modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9186489-7678-4531-b771-cbc5315d2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if a GPU is available in the current runtime and define `device`.\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(filters)\n",
    "importlib.reload(modules)\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8acd5-525a-4ecd-9bca-7d342dda9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This repository includes a small dataset of example sound files\n",
    "(speech waveforms stored as wav files), which can be loaded and\n",
    "played in the notebook.\n",
    "\"\"\"\n",
    "\n",
    "list_filename = glob.glob(\"data/*.wav\")\n",
    "print(f\"Found {len(list_filename)} wav files in the data directory\")\n",
    "filename = list_filename[-1]\n",
    "x, sr = sf.read(filename)\n",
    "print(f\"Loaded `{filename}`: {x.shape=}, {x.dtype=}, {sr=} Hz\")\n",
    "ipd.display(ipd.Audio(rate=sr, data=x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a68559",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "74a68559",
    "outputId": "c059ba92-2a65-4213-d753-1860f423d1c9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The provided `utils` module contains helper functions for manipulating\n",
    "and visualizing audio signals. Use the `utils.make_spectrogram_plot` to\n",
    "plot the sound waveform and spectrogram of the file loaded above.\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = utils.make_spectrogram_plot(\n",
    "    x,\n",
    "    sr,\n",
    "    figsize=(8, 4),\n",
    "    # nfft=512,\n",
    "    str_title=\"Spectrogram of example sound waveform\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bb830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "id": "856bb830",
    "outputId": "9ccd11d2-0e09-4cd5-d356-76ab07f86fdb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The `utils` module also contains helper functions for generating\n",
    "simple audio signals. Here, we generate a harmonic complex tone\n",
    "and plot its power spectrum.\n",
    "\"\"\"\n",
    "\n",
    "sr = 20e3\n",
    "x = utils.harmonic_complex_tone(\n",
    "    sr=sr,\n",
    "    dur=0.5,\n",
    "    f0=220,\n",
    "    phase=\"sine\",\n",
    "    harmonics=np.arange(1, 11),\n",
    "    amplitudes=1,\n",
    ")\n",
    "x = utils.set_dbspl(x, 60.0)\n",
    "\n",
    "ipd.display(ipd.Audio(rate=sr, data=x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fxx, pxx = utils.periodogram(x, sr=sr, scaling=\"spectrum\")\n",
    "ax.plot(fxx, pxx, color=\"k\", lw=1)\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    str_title=\"Harmonic complex tone\",\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    xscale=\"log\",\n",
    "    yscale=\"linear\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-60, None],\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# fig, ax = utils.make_periodogram_plot(x, sr, str_title=\"Harmonic complex tone\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46659a88-e074-4b04-abdd-fc57cbafc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "So far, we have been considering sound waveforms as NumPy arrays, which are\n",
    "the fundamental datatype for numerical processing in Python. The fundamental\n",
    "datatype for PyTorch is the tensor. Tensors are similar to NumPy arrays, but\n",
    "include support for GPU-acceleration and automatic differentiation.\n",
    "Converting between NumPy arrays and Torch tensors is easy, but requires some\n",
    "attention to device placement and datatypes.\n",
    "\n",
    "The PyTorch documentation contains additional information about tensors:\n",
    "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "impulse_numpy = np.zeros(int(1.0 * sr)) # Initialize 1 second of zeros\n",
    "impulse_numpy[0] = 1 # Set the first value to be 1 to make an impulse\n",
    "\n",
    "# When defining a tensor, it is good practice to specify the device and the datatype\n",
    "impulse_torch = torch.as_tensor(impulse_numpy, device=device, dtype=torch.float32)\n",
    "print(impulse_torch)\n",
    "\n",
    "impulse_torch.detach().cpu().numpy() # Convert a tensor to a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ab261",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "053ab261",
    "outputId": "fc4ffaa8-4cee-456b-e180-2f88e1290943"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensors can be be processed with PyTorch functions and modules. In the\n",
    "`modules.py` script, I have implemented a `GammatoneFilterbank` class,\n",
    "which we will use as the first stage of our simple auditory nerve model.\n",
    "\n",
    "Because PyTorch is designed for parallel numerical processing, this\n",
    "filterbank (like most models in PyTorch) expects batched rather than\n",
    "single inputs:\n",
    "    x.shape = [time] <-- single input sound waveform\n",
    "    x.shape = [batch, time] <-- batch of input sound waveforms\n",
    "\n",
    "To run a single sound waveform through a model, simply add a batch axis:\n",
    "    x = torch.zeros(n) <-- has shape [n]\n",
    "    x = x[None, ...] <-- has shape [1, n]\n",
    "\n",
    "Alternatively to this odd `[None, ...]` notation, you can use unsqueeze:\n",
    "    x = torch.zeros(n) <-- has shape [n]\n",
    "    x = x.unsqueeze(dim=0) <-- has shape [1, n]\n",
    "\"\"\"\n",
    "\n",
    "# Construct a filterbank object with specified parameters\n",
    "filterbank = modules.GammatoneFilterbank(\n",
    "    sr=sr,\n",
    "    fir_dur=0.05,\n",
    "    cfs=utils.erbspace(8e1, 8e3, 50),\n",
    "    dtype=torch.float32,\n",
    "    bw_mult=1,\n",
    ")\n",
    "# Assign it to a device\n",
    "filterbank = filterbank.to(device)\n",
    "\n",
    "# Apply the filterbank to our impulse tensor (note we added a batch axis)\n",
    "impulse_response_torch = filterbank(impulse_torch[None, ...])\n",
    "print(impulse_response_torch.shape)\n",
    "# And remove the added batch axis to get an output with shape [freq, time]\n",
    "impulse_response_torch = impulse_response_torch[0, ...]\n",
    "print(impulse_response_torch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea41a41-4ae4-4e6d-b99d-b371d58da175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the power spectra of the filterbank's impulse responses.\n",
    "The provided `utils.make_periodogram_plot` will plot the power\n",
    "spectrum of each frequency channel if the input is provided\n",
    "with shape [frequency_channels, time].\n",
    "\"\"\"\n",
    "\n",
    "# First, convert the impulse response tensor to a NumPy array\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "print(impulse_response.shape)\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    impulse_response,\n",
    "    sr,\n",
    "    str_title=\"Gammatone filterbank impulse responses\",\n",
    "    ylimits=[-30, None],\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf4de5-0096-4659-8988-d9994002cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This linear filterbank approximates the cochlea's frequency selectivity.\n",
    "The input represents the instantaneous pressure of a sound waveform and the\n",
    "output represents the instantaneous displacement of the basilar membrane \n",
    "at different points along its length (the \"cochlear frequency axis\").\n",
    "\n",
    "We can pass arbitrary sounds through this filterbank. Here, we filter\n",
    "white noise and plot the power spectra of the resulting subbands.\n",
    "\"\"\"\n",
    "\n",
    "x = torch.randn(1, int(sr), device=device, dtype=torch.float32)\n",
    "y = filterbank(x)[0].detach().cpu().numpy()\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    y,\n",
    "    sr,\n",
    "    str_title=\"White noise filtered by the gammatone filterbank\",\n",
    "    ylimits=[-30, None],\n",
    "    xlimits=[0, sr / 2],\n",
    "    xscale=\"linear\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1f8c0-e0d4-4c79-b097-6b594f9e4d82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "b1a1f8c0-e0d4-4c79-b097-6b594f9e4d82",
    "outputId": "b356276a-e0f5-473b-dafd-99722b06a39c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The next stage of our peripheral auditory model is a very crude model of\n",
    "inner hair cell transduction. Here, we simply use half-wave rectification\n",
    "to abstractly convert basilar membrane displacement to inner hair cell\n",
    "membrane potential.\n",
    "\n",
    "In NumPy / Matlab, half-wave rectification is easily implemented as:\n",
    "    x[x < 0] = 0  or  x = max(x, 0)\n",
    "In PyTorch, to avoid disrupting gradient tracking, we will use the\n",
    "provided \"rectified linear unit\" class `torch.nn.ReLU`.\n",
    "\"\"\"\n",
    "\n",
    "half_wave_rectification = torch.nn.ReLU()\n",
    "x = np.arange(-9, 10)\n",
    "y = half_wave_rectification(torch.as_tensor(x)).detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.axvline(0, color=\"k\", lw=0.2)\n",
    "ax.axhline(0, color=\"k\", lw=0.2)\n",
    "ax.axis(\"square\")\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    xlimits=[-10, 10],\n",
    "    ylimits=[-10, 10],\n",
    "    str_xlabel=\"Basilar membrane displacement (a.u.)\",\n",
    "    str_ylabel=\"Inner hair cell membrane potential (a.u.)\",\n",
    "    str_title=\"IHC transduction crudely modeled\\nas half-wave rectification (ReLU)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a13ca5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "07a13ca5",
    "outputId": "3a1f179f-a10a-47bc-ef7b-9cc93b070080"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, we apply a low-pass filter to the half-wave rectified subbands.\n",
    "This filter models the sluggishness of the inner hair cell membrane\n",
    "potential, which cannot oscillate as quickly the basilar membrane.\n",
    "This low-pass filter limits the temporal resolution of downstream\n",
    "auditory nerve fiber spiking (i.e., frequency limit of phase locking).\n",
    "\n",
    "Here we plot the filter's impulse response. The `modules.IHCLowpassFilter`\n",
    "module operates on inputs with shape [batch, time] or [batch, freq, time].\n",
    "In the latter case, the same filter will be applied along the time axis to\n",
    "each frequency channel.\n",
    "\"\"\"\n",
    "\n",
    "ihc_lowpass_filter = modules.IHCLowpassFilter(\n",
    "    sr_input=sr,\n",
    "    sr_output=sr,\n",
    "    fir_dur=0.05,\n",
    "    cutoff=3e3,\n",
    "    order=7,\n",
    ").to(device)\n",
    "\n",
    "impulse_response_torch = ihc_lowpass_filter(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "print(impulse_response.shape)\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    impulse_response,\n",
    "    sr,\n",
    "    str_title=\"Low-pass filter to limit auditory nerve phase locking\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309fb89-0217-457e-aa81-9e19abc35989",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "5309fb89-0217-457e-aa81-9e19abc35989",
    "outputId": "4b44649b-5ac2-4a1a-c69f-238d9cfdee49"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The last stage of the simple peripheral model will convert the half-wave\n",
    "rectified and low-pass filtered subbands to instantaneous auditory nerve\n",
    "firing rates. This is accomplished with a sigmoid function whose threshold\n",
    "and dynamic range can be set in dB SPL. Applying a sigmoid function at the\n",
    "output lets us crudely model loss of audibility at low sound levels and\n",
    "saturation at high sound levels.\n",
    "\n",
    "The parameters of this rate-level function are somewhat arbitrary and \n",
    "ignore much of the complexity of real auditory nerve fibers. Here, we\n",
    "will make some simplifying (and incorrect) assumptions about healthy\n",
    "auditory nerve fibers:\n",
    "- Threshold = 0 dB SPL\n",
    "- Dynamic range = 80 dB\n",
    "- Spontaneous activity = 0 spikes/s\n",
    "- Maximum activity = 250 spikes/s\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rate_level_function = modules.SigmoidRateLevelFunction(\n",
    "    rate_spont=0.0,\n",
    "    rate_max=250.0,\n",
    "    threshold=0.0,\n",
    "    dynamic_range=80.0,\n",
    "    dynamic_range_interval=0.95,\n",
    "    dtype=torch.float32,\n",
    ").to(device)\n",
    "x_db = np.arange(-20, 121)\n",
    "x_pa = 20e-6 * (10 ** (x_db / 20))\n",
    "x_pa_torch = torch.as_tensor(\n",
    "    x_pa,\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "y_torch = rate_level_function(x_pa_torch[None, None, :])[0, 0, :]\n",
    "y = y_torch.detach().cpu().numpy()\n",
    "ax.plot(x_db, y)\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    str_xlabel=\"Sound pressure level (dB SPL)\",\n",
    "    str_ylabel=\"Auditory nerve firing rate (spike/s)\",\n",
    "    str_title=\"Sigmoid rate-level function (healthy ear)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed535c1f-f684-45c9-9053-75fdebdc575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the healthy ear, outer hair cells (OHCs) act as a cochlear amplifier,\n",
    "boosting responses to quiet sounds. Sensorineural hearing loss often\n",
    "involves the death or dysfunction of OHCs, resulting in elevated thresholds\n",
    "and a reduced dynamic range. We can simulate these effects by adjusting\n",
    "the rate-level function parameters.\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "healthy_threshold = 0.0\n",
    "healthy_dynamic_range = 80.0\n",
    "for dbhl in np.arange(0, 75, 10):\n",
    "    rate_level_function = modules.SigmoidRateLevelFunction(\n",
    "        rate_spont=0.0,\n",
    "        rate_max=250.0,\n",
    "        threshold=healthy_threshold + dbhl,\n",
    "        dynamic_range=healthy_dynamic_range - dbhl,\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    x_db = np.arange(-20, 121)\n",
    "    x_pa = 20e-6 * (10 ** (x_db / 20))\n",
    "    x_pa_torch = torch.as_tensor(\n",
    "        x_pa,\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    y_torch = rate_level_function(x_pa_torch[None, None, :])[0, 0, :]\n",
    "    y = y_torch.detach().cpu().numpy()\n",
    "    ax.plot(x_db, y, label=f\"{dbhl} dB HL\")\n",
    "ax.legend(title=\"Hearing loss\")\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    str_xlabel=\"Sound pressure level (dB SPL)\",\n",
    "    str_ylabel=\"Auditory nerve firing rate (spike/s)\",\n",
    "    str_title=\"Sigmoid rate-level function (loss of cochear amplification)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22d3ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "6a22d3ee",
    "outputId": "e1fe32e6-39b9-42fd-ef46-b8024ad50437"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have examined each of the cochlear model stages individually,\n",
    "it is time to assemble the full cochlear model into a single torch Module.\n",
    "\n",
    "A skeleton for the `CochlearModel` class is provided below. If you are\n",
    "familiar with Python classes, you will notice the skeleton is subclassing\n",
    "the `torch.nn.Module` class. This class is the base class for all models in\n",
    "PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "Because our model is a subclass of `torch.nn.Module`, we only need to\n",
    "implement two methods (`__init__` and `forward`) to obtain a functioning\n",
    "PyTorch model. The `__init__` method sets up the model and is where you\n",
    "should define each of the model components. The `forward` method defines\n",
    "the computations that should be performed each time the model is called.\n",
    "Here, the `forward` pass should successively apply the model components\n",
    "defined in the `__init__` method.\n",
    "\n",
    "The convenience of PyTorch is that if we want to compute gradients from\n",
    "or through our model, we do not need to implement that `backward` pass\n",
    "ourselves. The derivatives will all be computed / tracked automatically!\n",
    "\"\"\"\n",
    "\n",
    "class CochlearModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sr_input=20000,\n",
    "        sr_output=10000,\n",
    "        fir_dur=0.05,\n",
    "        cfs=utils.erbspace(8e1, 8e3, 100),\n",
    "        bw_mult=1.0,\n",
    "        threshold=0.0,\n",
    "        dynamic_range=80.0,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple cochlear model in PyTorch.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        sr_input (int): sampling rate of the input sound waveform\n",
    "        sr_output (int): sampling rate of the output representation\n",
    "        fir_dur (float): duration of finite impulse responses for filtering (s)\n",
    "        cfs (np.ndarray): characteristic frequencies of the cochlear filters (Hz)\n",
    "        bw_mult (float): scales cochlear filter bandwidths\n",
    "        threshold (float): absolute threshold of auditory nerve fibers (dB SPL)\n",
    "        dynamic_range (float): auditory nerve fiber dynamic range (dB)\n",
    "        dtype (torch.dtype): datatype for internal intensors and inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sr_input = sr_input\n",
    "        self.sr_output = sr_output\n",
    "        self.cfs = cfs\n",
    "        self.cochlear_filterbank = modules.GammatoneFilterbank(\n",
    "            sr=sr_input,\n",
    "            fir_dur=fir_dur,\n",
    "            cfs=self.cfs,\n",
    "            dtype=dtype,\n",
    "            bw_mult=bw_mult,\n",
    "        )\n",
    "        self.half_wave_rectification = torch.nn.ReLU()\n",
    "        self.ihc_lowpass_filter = modules.IHCLowpassFilter(\n",
    "            sr_input=sr_input,\n",
    "            sr_output=sr_output,\n",
    "            fir_dur=fir_dur,\n",
    "            cutoff=3e3,\n",
    "            order=7,\n",
    "        )\n",
    "        self.rate_level_function = modules.SigmoidRateLevelFunction(\n",
    "            rate_spont=0.0,\n",
    "            rate_max=250.0,\n",
    "            threshold=threshold,\n",
    "            dynamic_range=dynamic_range,\n",
    "            dynamic_range_interval=0.95,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run the cochlear model on an input sound waveform.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x (torch.Tensor): input sound waveform with shape [batch, time]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x (torch.Tensor): output of cochlear model with shape [batch, freq, time]\n",
    "        \"\"\"\n",
    "        x = self.cochlear_filterbank(x)\n",
    "        x = self.half_wave_rectification(x)\n",
    "        x = self.ihc_lowpass_filter(x)\n",
    "        x = self.half_wave_rectification(x)\n",
    "        x = self.rate_level_function(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4728d-1b18-4c04-ab9c-ed919495e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the cochlear model implementation with this cell.\n",
    "\n",
    "(1) Construct a cochlear_model object with a call to the `CochlearModel` class\n",
    "(2) Generate or load a sound waveform `x`\n",
    "(3) Convert `x` to a tensor with shape [batch=1, time]\n",
    "(3) Apply cochlear_model to `x` to produce `x_nervegram` with shape [batch=1, freq, time]\n",
    "(4) Convert `x_nervegram` to a NumPy array with shape [freq, time]\n",
    "(5) Visualize `x_nervegram` using `utils.make_nervegram_plot`\n",
    "\"\"\"\n",
    "\n",
    "cochlear_model = CochlearModel(\n",
    "    sr_input=20e3,\n",
    "    sr_output=10e3,\n",
    "    cfs=utils.erbspace(8e1, 8e3, 100),\n",
    ").to(device)\n",
    "\n",
    "x = utils.harmonic_complex_tone(\n",
    "    sr=cochlear_model.sr_input,\n",
    "    dur=0.05,\n",
    "    f0=220,\n",
    "    phase=\"sine\",\n",
    "    harmonics=np.arange(1, 31, 1),\n",
    ")\n",
    "x = utils.set_dbspl(x, 60)\n",
    "# x, _ = sf.read(\"data/000.wav\")\n",
    "\n",
    "x_tensor = torch.as_tensor(x, device=device).float()[None, ...]\n",
    "x_nervegram_tensor = cochlear_model(x_tensor)[0]\n",
    "x_nervegram = x_nervegram_tensor.detach().cpu().numpy()\n",
    "print(x_nervegram.shape)\n",
    "\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram,\n",
    "    sr_waveform=cochlear_model.sr_input,\n",
    "    sr_nervegram=cochlear_model.sr_output,\n",
    "    cfs=cochlear_model.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003b9da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9003b9da",
    "outputId": "68295ff1-ce01-422a-e873-fc7f839640b8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get a sense for how fast the cochlear model can run on the current device.\n",
    "\"\"\"\n",
    "\n",
    "progress_bar = tqdm.tqdm(range(20)) # tqdm.tqdm wraps an iterable and displays a progress bar\n",
    "for itr in progress_bar:\n",
    "    x_nervegram_tensor = cochlear_model(x_tensor)\n",
    "    progress_bar.set_postfix({\"current_iteration\": itr})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e0394-e80e-43cc-9473-f56de52d2cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "db7e0394-e80e-43cc-9473-f56de52d2cc8",
    "outputId": "d070d591-f462-4de8-aa7c-76165add1b34"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize the effect of hearing loss on the\n",
    "simulated auditory nerve representations.\n",
    "\"\"\"\n",
    "\n",
    "cfs = utils.erbspace(8e1, 8e3, 100)\n",
    "\n",
    "# Construct a healthy cochlear model\n",
    "cochlear_model_healthy = CochlearModel(\n",
    "    sr_input=sr,\n",
    "    cfs=cfs,\n",
    "    threshold=0.0,\n",
    "    dynamic_range=80.0,\n",
    ").to(device)\n",
    "\n",
    "# Construct an impaired cochlear model\n",
    "cochlear_model_impaired = CochlearModel(\n",
    "    sr_input=sr,\n",
    "    cfs=cfs,\n",
    "    threshold=60.0,\n",
    "    dynamic_range=20.0,\n",
    ").to(device)\n",
    "\n",
    "# Define a stimulus to play to both cochlear models\n",
    "x = utils.harmonic_complex_tone(\n",
    "    sr=cochlear_model_healthy.sr_input,\n",
    "    dur=0.05,\n",
    "    f0=220,\n",
    "    phase=\"sine\",\n",
    "    harmonics=np.arange(1, 31, 1),\n",
    ")\n",
    "x = utils.set_dbspl(x, 70)\n",
    "\n",
    "# Run the stimulus through both cochlear models\n",
    "x_tensor = torch.as_tensor(x, device=device, dtype=torch.float32)[None, ...]\n",
    "x_nervegram_healthy = cochlear_model_healthy(x_tensor)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired = cochlear_model_impaired(x_tensor)[0].detach().cpu().numpy()\n",
    "\n",
    "# Visualize both auditory nerve representations\n",
    "print(\"Healthy cochlear model\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_healthy,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_impaired,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52256970-50ce-4c2d-b183-8dd70799de5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "52256970-50ce-4c2d-b183-8dd70799de5a",
    "outputId": "de25e0a0-eac2-45c0-a91a-794e7397f9b0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A reasonable objective for a hearing aid would be to process audio\n",
    "in such a way that the auditory nerve representation of an impaired\n",
    "ear is maximally similar to that of a healthy ear. How can we\n",
    "implement this notion quantitatively, as a loss function?\n",
    "\n",
    "x <-- unprocessed sound waveform\n",
    "x_aided = hearing_aid(x)\n",
    "y_healthy = cochlear_model_healthy(x)\n",
    "y_impaired = cochlear_model_impaired(x_aided)\n",
    "Goal: minimize distance_between(y_healthy, y_impaired)\n",
    "\n",
    "Some of the useful PyTorch loss functions include:\n",
    "    torch.nn.functional.mse_loss\n",
    "    torch.nn.functional.l1_loss\n",
    "(https://pytorch.org/docs/stable/nn.functional.html#loss-functions)\n",
    "\"\"\"\n",
    "\n",
    "def loss_function(x, x_aided):\n",
    "    y_healthy = cochlear_model_healthy(x)\n",
    "    y_impaired = cochlear_model_impaired(x_aided)\n",
    "    loss = torch.nn.functional.mse_loss(y_impaired, y_healthy)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcaaed-fb81-48be-97a7-f603eb823f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will first consider the simplest possible hearing aid: \n",
    "linear amplification applied to the entire sound waveform.\n",
    "\n",
    "Here there is just one parameter to optimize (the gain), so\n",
    "we can easily find the optimal value with brute force.\n",
    "\"\"\"\n",
    "\n",
    "list_db_gain = np.arange(0, 101, 5)\n",
    "list_loss = []\n",
    "for db_gain in tqdm.tqdm(list_db_gain):\n",
    "    amp = np.power(10, db_gain / 20)\n",
    "    loss = loss_function(\n",
    "        x=x_tensor,\n",
    "        x_aided=amp * x_tensor,\n",
    "    ).item()\n",
    "    list_loss.append(loss)\n",
    "\n",
    "best_db_gain = list_db_gain[np.argmin(list_loss)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list_db_gain, list_loss, marker=\".\", color=\"r\", ls=\"-\")\n",
    "ax.plot(\n",
    "    best_db_gain,\n",
    "    np.min(list_loss),\n",
    "    marker=\"o\",\n",
    "    ms=12,\n",
    "    ls=\"\",\n",
    "    mfc=\"none\",\n",
    "    color=\"k\",\n",
    "    label=f\"Optimal gain = {best_db_gain} dB\",\n",
    ")\n",
    "ax.legend()\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    str_title=\"Brute force hearing aid optimization\",\n",
    "    str_xlabel=\"Hearing aid parameter (gain in dB)\",\n",
    "    str_ylabel=\"Loss function (MSE between healthy and\\nimpaired auditory nerve representation)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4071a-a31a-4b4a-bfef-c5646e4b4e27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "7de4071a-a31a-4b4a-bfef-c5646e4b4e27",
    "outputId": "8dc6169b-61cf-4407-f9a3-75936a571ede"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize the resulting auditory nerve representations.\n",
    "\"\"\"\n",
    "\n",
    "best_amp = np.power(10, best_db_gain / 20)\n",
    "\n",
    "x_tensor = torch.as_tensor(x, device=device, dtype=torch.float32)[None, ...]\n",
    "x_nervegram_healthy = cochlear_model_healthy(x_tensor)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired = cochlear_model_impaired(best_amp * x_tensor)[0].detach().cpu().numpy()\n",
    "\n",
    "print(\"Healthy cochlear model with unprocessed input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_healthy,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model with aided input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_impaired,\n",
    "    sr_waveform=cochlear_model_impaired.sr_input,\n",
    "    sr_nervegram=cochlear_model_impaired.sr_output,\n",
    "    cfs=cochlear_model_impaired.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13f0e3-3a9f-43f4-9e8d-090c361b9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Real hearing losses are rarely uniform across all frequencies.\n",
    "How can we compensate for a more plausible frequency-dependent\n",
    "hearing loss?\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for severity in [\"ref\", \"mild\", \"moderate\", \"moderate_severe\"]:\n",
    "    freq, dbhl = utils.get_example_audiogram(severity)\n",
    "    ax.plot(freq, dbhl, marker=\"o\", label=severity)\n",
    "ax.legend(title=\"Hearing loss severity\")\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    ylimits=[90, -10],\n",
    "    xticks=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticklabels=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticks_minor=[],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Hearing threshold elevation (dB HL)\",\n",
    "    str_title=\"Representative audiograms\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2fa0b-e9f6-412c-960a-ac1b24558390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Construct a new impaired cochlear model that better approximates\n",
    "a realistic hearing-impaired audiogram. Here we set the model's\n",
    "`threshold` and `dynamic_range` as frequency-specific arrays with\n",
    "the same shape as `cfs`.\n",
    "\"\"\"\n",
    "\n",
    "freq, dbhl = utils.get_example_audiogram(severity=\"moderate_severe\")\n",
    "threshold, dynamic_range = utils.map_audiogram_to_rate_level_parameters(\n",
    "    freq=freq,\n",
    "    dbhl=dbhl,\n",
    "    cfs=cfs,\n",
    "    healthy_threshold=healthy_threshold,\n",
    "    healthy_dynamic_range=healthy_dynamic_range,\n",
    ")\n",
    "print(f\"{cfs.shape=}, {threshold.shape=}, {dynamic_range.shape=}\")\n",
    "cochlear_model_impaired = CochlearModel(\n",
    "    sr_input=sr,\n",
    "    cfs=cfs,\n",
    "    threshold=threshold,\n",
    "    dynamic_range=dynamic_range,\n",
    ").to(device)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(freq, dbhl, marker=\"o\", color=\"k\", label=\"Example audiogram\")\n",
    "ax.plot(\n",
    "    cochlear_model_impaired.cfs,\n",
    "    cochlear_model_impaired.rate_level_function.threshold,\n",
    "    marker=\".\",\n",
    "    color=\"r\",\n",
    "    label=\"Rate-level function thresholds at each CF\",\n",
    ")\n",
    "ax.legend()\n",
    "ax = utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    ylimits=[90, -10],\n",
    "    xticks=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticklabels=[125, 250, 500, 1000, 2000, 4000, 8000, 16000],\n",
    "    xticks_minor=[],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Hearing threshold (dB HL)\",\n",
    "    str_title=\"Setting model parameters to match an audiogram\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb910c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "73cb910c",
    "outputId": "a80bc177-09dc-4045-8288-c340ba567748"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One possibility is to build a hearing aid with frequency-specific\n",
    "gains. We can filter the input sound into different frequency\n",
    "channels, scale the channels with separate gains, and then sum\n",
    "the channels back together.\n",
    "\"\"\"\n",
    "\n",
    "filterbank = modules.HalfCosineFilterbank(\n",
    "    sr=sr,\n",
    "    cf_low=20,\n",
    "    cf_high=8000,\n",
    "    cf_num=7,\n",
    "    scale=\"log\",\n",
    "    include_highpass=True,\n",
    "    include_lowpass=True,\n",
    ").to(device)\n",
    "\n",
    "impulse_response_torch = filterbank(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "print(impulse_response.shape)\n",
    "\n",
    "fig, ax = utils.make_periodogram_plot(\n",
    "    impulse_response,\n",
    "    sr,\n",
    "    str_title=\"Half-cosine filterbank\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7faf8c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "d7faf8c4",
    "outputId": "4fccba92-4837-4562-b43a-106edff0f7bd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Such a hearing aid would have multiple tunable parameters, and the brute\n",
    "force method of finding optimal values rapidly becomes intractable. We\n",
    "will instead construct the hearing aid as a PyTorch model with learnable\n",
    "parameters. We can then use gradient descent to efficiently find values\n",
    "that minimize our loss function.\n",
    "\"\"\"\n",
    "\n",
    "class HearingAid(torch.nn.Module):\n",
    "    def __init__(self, sr=20e3):\n",
    "        \"\"\"\n",
    "        Initialize the hearing aid model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filterbank = modules.HalfCosineFilterbank(\n",
    "            sr=sr,\n",
    "            cf_low=20,\n",
    "            cf_high=8000,\n",
    "            cf_num=7,\n",
    "            scale=\"log\",\n",
    "            include_highpass=True,\n",
    "            include_lowpass=True,\n",
    "        )\n",
    "        self.gains = torch.nn.parameter.Parameter(\n",
    "            data=torch.zeros(self.filterbank.cf_num),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the computations to an input sound `x`.\n",
    "        \"\"\"\n",
    "        y = self.filterbank(x)\n",
    "        y = y * torch.pow(10, self.gains / 20).view(1, -1, 1)\n",
    "        y = torch.sum(y, axis=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "hearing_aid = HearingAid(sr=sr).to(device)\n",
    "# hearing_aid.gains = torch.nn.Parameter(5 * torch.randn(*hearing_aid.gains.shape))\n",
    "print(hearing_aid)\n",
    "for n, p in hearing_aid.named_parameters():\n",
    "    print(n, p if p.ndim == 1 else p.shape)\n",
    "\n",
    "impulse_response_torch = hearing_aid(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fxx, pxx = utils.periodogram(impulse_numpy, sr)\n",
    "fyy, pyy = utils.periodogram(impulse_response, sr)\n",
    "ax.plot(fxx, pxx, color=\"k\", label=\"Hearing aid input\")\n",
    "ax.plot(fyy, pyy, color=\"r\", label=\"Hearing aid output\")\n",
    "ax.legend()\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-10, None],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    str_title=\"Hearing aid transfer function before optimization\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4b082",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22e4b082",
    "outputId": "c6971a9c-9575-4bf3-8f38-b56ebef48ae3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimize the parameters of the hearing aid via gradient descent.\n",
    "\n",
    "This cell will run extremely slowly without GPU acceleration.\n",
    "If a GPU is not available, use a smaller dataset by truncating\n",
    "the number of examples and/or the length of the stimuli.\n",
    "\"\"\"\n",
    "\n",
    "# Define the dataset (for time, we use only a small batch of speech signals)\n",
    "x = np.stack(\n",
    "    [sf.read(\"data/{:03d}.wav\".format(_))[0] for _ in range(1)],\n",
    "    axis=0,\n",
    ")\n",
    "x = torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "print(f\"Dataset shape: {x.shape}\")\n",
    "\n",
    "# Define a PyTorch optimizer object and tell it which parameters to update\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=list(hearing_aid.parameters()),\n",
    "    lr=1e-1,  # The learning rate is a hyperparameter (determines gradient descent step size)\n",
    ")\n",
    "\n",
    "# In the optimization loop, we iteratively compute the loss,\n",
    "# calculate the gradients with `loss.backward()`, and then\n",
    "# call `optimizer.step()` to update the parameters.\n",
    "progress_bar = tqdm.tqdm(range(500))\n",
    "for step in progress_bar:\n",
    "    optimizer.zero_grad()\n",
    "    x_aided = hearing_aid(x)\n",
    "    loss = loss_function(x=x, x_aided=x_aided)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    progress_bar.set_postfix({\"loss\": \"{:0.4f}\".format(loss.item())})\n",
    "\n",
    "print(\"hearing_aid parameters after optimization:\")\n",
    "for n, p in hearing_aid.named_parameters():\n",
    "    print(n, p if p.ndim == 1 else p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7ecdf-dfa4-4884-b630-9d17fdea195f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4de7ecdf-dfa4-4884-b630-9d17fdea195f",
    "outputId": "324fd35d-cb54-438a-e6ba-57ca7f267543"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize the optimized hearing aid's signal processing.\n",
    "\"\"\"\n",
    "\n",
    "impulse_response_torch = hearing_aid(impulse_torch[None, ...])[0]\n",
    "impulse_response = impulse_response_torch.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fxx, pxx = utils.periodogram(impulse_numpy, sr)\n",
    "fyy, pyy = utils.periodogram(impulse_response, sr)\n",
    "ax.plot(fxx, pxx, color=\"k\", label=\"Hearing aid input\")\n",
    "ax.plot(fyy, pyy, color=\"r\", label=\"Hearing aid output\")\n",
    "ax.legend()\n",
    "utils.format_axes(\n",
    "    ax,\n",
    "    xscale=\"log\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-10, None],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    str_title=\"Hearing aid transfer function after optimization\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "example_audio = torch.as_tensor(\n",
    "    sf.read(\"data/000.wav\".format(_))[0][None, :],\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "example_audio_aided = hearing_aid(example_audio)\n",
    "example_audio = example_audio[0].detach().cpu().numpy()\n",
    "example_audio_aided = example_audio_aided[0].detach().cpu().numpy()\n",
    "\n",
    "fig, ax_arr = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "t = np.arange(0, len(example_audio)) / sr\n",
    "ax_arr[0].plot(t, example_audio, color=\"k\", alpha=0.5)\n",
    "ax_arr[0].plot(t, example_audio_aided, color=\"r\", alpha=0.5)\n",
    "ax_arr[0] = utils.format_axes(\n",
    "    ax_arr[0],\n",
    "    str_xlabel=\"Time (s)\",\n",
    "    str_ylabel=\"Pressure (Pa)\",\n",
    "    str_title=\"Time domain\",\n",
    ")\n",
    "fxx, pxx = utils.periodogram(example_audio, sr)\n",
    "fyy, pyy = utils.periodogram(example_audio_aided, sr)\n",
    "ax_arr[1].plot(fxx, pxx, color=\"k\", alpha=0.5)\n",
    "ax_arr[1].plot(fyy, pyy, color=\"r\", alpha=0.5)\n",
    "ax_arr[1] = utils.format_axes(\n",
    "    ax_arr[1],\n",
    "    xscale=\"log\",\n",
    "    xlimits=[10, sr / 2],\n",
    "    ylimits=[-10, None],\n",
    "    str_xlabel=\"Frequency (Hz)\",\n",
    "    str_ylabel=\"Power (dB SPL)\",\n",
    "    str_title=\"Frequency domain\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ipd.display(ipd.Audio(rate=sr, data=example_audio))\n",
    "ipd.display(ipd.Audio(rate=sr, data=example_audio_aided))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3a893",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b4b3a893",
    "outputId": "b69422f6-1ee4-4f22-eb66-c862e44d5b21"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize the resulting auditory nerve representations.\n",
    "\"\"\"\n",
    "\n",
    "# x = utils.harmonic_complex_tone(\n",
    "#     sr=sr,\n",
    "#     dur=0.05,\n",
    "#     f0=220,\n",
    "#     phase=\"sine\",\n",
    "#     harmonics=np.arange(1, 31),\n",
    "# )\n",
    "# x = utils.set_dbspl(x, 60.0)\n",
    "x, _ = sf.read(\"data/099.wav\")\n",
    "\n",
    "x = torch.as_tensor(x[None, ...], device=device, dtype=torch.float32)\n",
    "x_aided = hearing_aid(x)\n",
    "x_nervegram_healthy = cochlear_model_healthy(x)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired = cochlear_model_impaired(x)[0].detach().cpu().numpy()\n",
    "x_nervegram_impaired_aided = cochlear_model_impaired(x_aided)[0].detach().cpu().numpy()\n",
    "x = x[0].detach().cpu().numpy()\n",
    "x_aided = x_aided[0].detach().cpu().numpy()\n",
    "\n",
    "print(\"Healthy cochlear model with unprocessed input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_healthy,\n",
    "    sr_waveform=cochlear_model_healthy.sr_input,\n",
    "    sr_nervegram=cochlear_model_healthy.sr_output,\n",
    "    cfs=cochlear_model_healthy.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model with unprocessed input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x,\n",
    "    nervegram=x_nervegram_impaired,\n",
    "    sr_waveform=cochlear_model_impaired.sr_input,\n",
    "    sr_nervegram=cochlear_model_impaired.sr_output,\n",
    "    cfs=cochlear_model_impaired.cfs,\n",
    ")\n",
    "plt.show()\n",
    "print(\"Impaired cochlear model with aided input\")\n",
    "fig, ax_arr = utils.make_nervegram_plot(\n",
    "    waveform=x_aided,\n",
    "    nervegram=x_nervegram_impaired_aided,\n",
    "    sr_waveform=cochlear_model_impaired.sr_input,\n",
    "    sr_nervegram=cochlear_model_impaired.sr_output,\n",
    "    cfs=cochlear_model_impaired.cfs,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc744ec-5a35-4f02-a2a7-ed67bbde8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* * * Ideas for additional exploration * * *\n",
    "\n",
    "On first glance, the hearing aid with frequency-specific linear gains\n",
    "appears to do quite well. The impaired auditory nerve representation\n",
    "of the hearing-aid-processed signal reasonably resembles the healthy\n",
    "auditory nerve representation of the unprocessed signal.\n",
    "\n",
    "But it is far from perfect -- linear amplification cannot compensate\n",
    "for the reduced dynamic range of the impaired ear. Can you design and\n",
    "optimize a hearing aid that further minimizes the loss function?\n",
    "\n",
    "In addition to elevating thresholds and reducing dynamic ranges, loss\n",
    "of OHCs also causes leads to broader cochlear frequency tuning. This\n",
    "can be simulated by specifying `bw_mult` in `modules.GammatoneFilterbank`.\n",
    "Can you learn a hearing aid that helps compensate for this consequence\n",
    "of hearing loss?\n",
    "\n",
    "The \"hearing aids\" in this notebook are toy examples with simple signal\n",
    "processing and very small numbers of parameters. With a bit more compute\n",
    "power and a larger dataset, the same approach can be used to optimize more\n",
    "complex audio-processing systems, such as deep artificial neural networks.\n",
    "See `torchaudio.models` (https://pytorch.org/audio/main/models.html) for\n",
    "ready-to-use PyTorch models for common audio tasks.\n",
    "\n",
    "Here, our hearing aid optimization objective is to essentially restore\n",
    "healthy auditory nerve representations in a very simple model of the\n",
    "impaired auditory nerve. Is this a good \"loss function\"? How might we\n",
    "improve upon it?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b1474-992c-48f5-91c2-ebb115e5c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Another way to achieve frequency-specific linear gain.\n",
    "This hearing aid uses convolution in the time-domain\n",
    "with a learnable FIR filter kernel.\n",
    "\"\"\"\n",
    "\n",
    "class ConvolutionalHearingAid(torch.nn.Module):\n",
    "    def __init__(self, sr=20e3):\n",
    "        \"\"\"\n",
    "        Initialize the hearing aid model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = modules.AudioConv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=None,\n",
    "            sr=sr,\n",
    "            fir_dur=0.01,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the computations to an input sound `x`.\n",
    "        \"\"\"\n",
    "        y = self.conv(x.unsqueeze(1)).squeeze(1)\n",
    "        return y\n",
    "\n",
    "\n",
    "ConvolutionalHearingAid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24863f-6dcd-4657-b740-19529ccaa608",
   "metadata": {
    "id": "dc24863f-6dcd-4657-b740-19529ccaa608"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
